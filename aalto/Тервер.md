#aalto 
### Обозначения
![[обознач.png]]


### Теорема Байеса
$$
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
$$
- $P(A|B)$ - вероятность гипотезы A при наступлении события B

<br><br>

### Размещения без повторений 
$$
C(n, k) = \binom{n}{k} = \frac{n!}{k!(n-k)!}
$$
- **пример:**  
	Нужно выбрать 5 человек из 20 (порядок не имеет значения)

<br><br>

###  Математическое ожидание ($\mu$)
Для **дискретной** случайной величины $X$:
$$
E(X) = \sum_{x} x f_{x}(x) 
$$

Для **непрерывной** случайной величины $X$:
$$
E(X) = \int_{-\infty}^{+\infty} x f_{x}(x) dx
$$

- $f_{x}(x)$ - функция плотности вероятности. 

#### Свойства математического ожидания:

1. **Линейность**: Если $a$ и $b$ — константы, а $X$ и $Y$ — случайные величины, то: 
$$
E(aX + bY) = aE(X) + bE(Y)
$$
2. **Неотрицательность**: Если $X$ — неотрицательная случайная величина (то есть $X \geq 0$ с вероятностью $1$), то:
$$
E(X) \geq 0
$$
3. **Сумма независимых случайных величин**: Если $X$ и $Y$ — независимые случайные величины, то:
$$
E(X+Y) = E(X) + E(Y)
$$
4. Математическое ожидание **сложной функции** $g(X)$:
$$
E(g(X)) = \sum_{x} g(x) f_{X}(x) \hspace{20mm} E(g(X)) = \int_{-\infty}^{+\infty} g(x) f_{X}(x) dx
$$
5. Математическое ожидание **сложной функции от двух переменных** $g(X, Y)$:
$$
E(g(X, Y)) = \sum_{x} \sum_{y} g(x, y) f_{X, Y}(x, y) \\
$$
$$
E(g(X, Y)) = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} g(x, y) f_{X, Y}(x, y) dx dy
$$

<br><br>

### Дисперсия и cтандартное отклонение
$\text{Var}(X)$ - дисперсия  
$SD(X) = \sigma$ - cтандартное отклонение

$$
\sigma = \sqrt{\text{Var}(X)}
$$
$$
\sigma^{2} = \text{Var}(X) = \sum_{i} (x_{i} - \mu)^{2} \cdot f(x)
$$
$$
\text{Var}(x) = \int_{-\infty}^{+\infty} (x - \mu)^{2} \cdot f(x) dx
$$
$$
\text{Var}(X) = E(X^{2}) - (EX)^{2}
$$
$$
\text{Var}(X+Y) = \text{Var}(X) + 2\text{Cov}(X, Y) + \text{Var}(Y) 
$$
#### Свойства стандартного откланения:
$$
\text{SD}(1) = 0
$$
$$
\text{SD}(aX) = |a| \text{SD}(X)
$$
$$
\text{SD}(aX + b) = |a| \text{SD}(X)
$$

<br><br>

### Неравенство Чебышёва
Для любого $k>0$ выполняется неравенство:
$$
P(X = \mu \pm k \sigma) \geq 1 - \frac{1}{k^{2}}
$$

#### Пример:
Средняя зарплата сотрудников компании составляет 50 00050000 рублей, а стандартное отклонение зарплат равно 10 00010000 рублей. Оцените вероятность того, что случайно выбранный сотрудник имеет зарплату в пределах от 30 00030000 до 70 00070000 рублей.
$$
\begin{gather*}
\mu = 50000 \hspace{10mm} \sigma = 10000 \hspace{10mm} \text{интервал } [30000, 70000]\\
50000 \pm k \cdot 10000 = [30000, 70000] = 50000 \pm 20000 \Longrightarrow\\
k = 2 \hspace{10mm} P \geq 1- \frac{1}{k^{2}} = \frac{3}{4}
\end{gather*}
$$

<br><br>


### Ковариация и корреляция совместного распределения
**Ковариация** — это мера **линейной связи** между двумя случайными величинами XX и YY. Она показывает, как изменения одной величины связаны с изменениями другой.
$$
\begin{gather*}
\text{Cov}(X, Y) = E[(X-\mu_{X})(Y-\mu_{Y})] =\\
\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} (x- \mu_{X}) (y- \mu_{Y}) f_{X, Y}(x, y) dx dy\\
\\ \\
\text{Cov}(X, Y) = E(XY) - E(x)E(Y)
\end{gather*}
$$

#### Свойства ковариации:
- $\text{Cov}(X, Y) = \text{Cov}(Y, X)$
- $\text{Cov}(aX + b, cY + d) = ac \cdot \text{Cov}(X, Y)$
- Если $X$ и $Y$ независимы, то $\text{Cov}(X, Y) = 0$. **Обратное не всегда верно!**

**Корреляция** — это нормированная мера линейной связи между двумя случайными величинами. Она всегда лежит в диапазоне от −1−1 до 11.
$$
\rho_{X, Y} = \frac{\text{Cov}(X, Y)}{\sigma_{X} \cdot \sigma_{Y}}
$$
- $\sigma_{X}$ - стандартное отклонение $X$
- $\sigma_{Y}$ - стандартное отклонение $Y$
#### Свойства корреляции:
- $\rho_{X, Y} \in [-1, 1]$
- Если $\rho_{X, Y} = 1$, то между $X$ и $Y$ существует **положительная линейная зависимость**.
- Если $\rho_{X, Y} = -1$, то между $X$ и $Y$ существует **отрицательная линейная зависимость**
- Если $\rho_{X, Y} = 0$, то линейная связь отсутствует (но может существовать нелинейная зависимость).

<br><br>

### Формула Бернулли
Вероятность того, что произойдет ровно $k$ успехов в $n$ испытаниях, задается **формулой Бернулли** (наример какова вероятность, что из 5 брасков, 3 будут решки)
$$
P(X=k) = \binom{n}{k} p^k q^{n-k}
$$

<br><br>

### Сумма случайных величин
$$
\text{Var}(X+Y) = \text{Var}(X) + 2\text{Cov}(X, Y) + \text{Var}(Y) 
$$

Если $X$ и $Y$ независимы, то $\text{Cov}(X, Y) = 0$
$$
\begin{gather*}
=\text{Var}(X)+\text{Var}(Y) = \sigma_{X}^{2} + \sigma_{Y}^{2}\\
\sigma_{X+Y} = \sqrt{\sigma_{X}^{2} + \sigma_{Y}^{2}} \\
\\
\text{SD}\left( \sum_{i=1}^{n} X_{i} \right) = \sigma \sqrt{n}\\
\text{E}\left( \sum_{i=1}^{n} X_{i} \right) = \mu n
\end{gather*}
$$
#### Пример:
Играем $n$ раундов в игру с кубиком, где в каждом раунде выигрывается сумма, равная количеству очков на кубике. Вычислите ожидаемое значение накопленного выигрыша $S=X_{1} + X_{2} + ... + X_{n}$ и стандартное отклонение в случаях, когда $n=10$, $n=100$, $n=1000$.
$$
\begin{gather*}
E(X) = \frac{1+2+3+4+5+6}{6} = 3.5 \hspace{10mm} SD(X) = \sqrt{ E(X^{2}) -(E(X))^{2}} = \sqrt{ \frac{91}{6} - (3.5)^{2}} = \sqrt{\frac{35}{12}}\\

n=10, \hspace{10mm} E(S) = 10 \cdot 3.5 = 35 \hspace{10mm} SD(S) = \sqrt{ 10 \cdot \frac{35}{12} } \approx 5.4\\

n=100, \hspace{10mm} E(S) = 100 \cdot 3.5 = 350 \hspace{10mm} SD(S) = \sqrt{ 100 \cdot \frac{35}{12} } \approx 17.1\\

n=1000, \hspace{10mm} E(S) = 1000 \cdot 3.5 = 3500 \hspace{10mm} SD(S) = \sqrt{ 1000 \cdot \frac{35}{12} } \approx 54.0\\
\end{gather*}
$$

<br><br>

### Нормальное распределение $\mathcal{N}(\mu, \sigma^{2})$
$$
f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}
$$
- $f(x)$ — функция плотности вероятности нормального распределения,
- $\mu$ — математическое ожидание (среднее значение),
- $\sigma$ — стандартное отклонение,
- $e$ — основание натурального логарифма.
- [[normaalijakauma.pdf]]

График функции плотности нормального распределения имеет форму **колокола** (его часто называют "колоколом Гаусса"). Он симметричен относительно среднего значения $\mu$.

#### Свойства нормального распределения
- **Пик** графика находится в точке $x=\mu$.
- **Ширина** графика зависит от стандартного отклонения $\sigma$: чем больше $\sigma$, тем шире и ниже колокол.
- **Правило трёх сигм**:
    
    1. В пределах $\mu \pm \sigma$ находится около $68\%$ значений. 
    2. В пределах $\mu \pm 2 \sigma$ находится около $95\%$ значений.
    3. В пределах $\mu \pm 3 \sigma$ находится около $99.7\%$ значений.
- **Центральная предельная теорема (CLT)** утверждает, что сумма большого числа независимых и одинаково распределённых случайных величин стремится к нормальному распределению.
$$
\frac{S_{n} - n \mu}{\sigma \sqrt{ n }} \sim \mathcal{N}(0, 1), \hspace{10mm} n \to \infty
$$
- **Стандартное нормальное распределение** — это частный случай нормального распределения, где $\mu = 0$ и $\sigma = 1$. Его функция плотности:
$$
f(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}
$$
![[СтандартНормРаспред.png]]

#### Пример
##### Условие:
Рост взрослых мужчин в некоторой стране распределён нормально со средним значением $\mu = 175$ см и стандартным отклонением $\sigma = 10$ см. Найдите:

1. Вероятность того, что случайно выбранный мужчина имеет рост:
    - Меньше 160 см.
    - Больше 190 см.
    - От 170 до 180 см.
        
2. Рост, который превышает 90% мужчин.

##### Решение:
1. *Вероятность того, что рост меньше 160 см.*
$$
z = \frac{X-\mu}{\sigma} = \frac{160 - 175}{10} = -1.5
$$
**Используем таблицу стандартного нормального распределения** (или калькулятор) для нахождения $P(Z<-1.5)$:
$$
P(Z<-1.5) \approx 0.0668
$$
**Ответ:** Вероятность того, что рост меньше 160 см, составляет около 6.68%.

2. *Вероятность того, что рост больше 190 см.*
$$
z = \frac{190 - 175}{10} = 1.5
$$
**Используем таблицу для нахождения** $P(Z>1.5)$
$$
P(Z>1.5) = 1 - P(Z<1.5) = 1 - 0.9332 = 0.0668 
$$
**Ответ:** Вероятность того, что рост больше 190 см, составляет около 6.68%.

3. *Вероятность того, что рост находится между 170 и 180 см.*
$$
\begin{gather*}
z_{1} = \frac{170 - 175}{10} = -0.5 \hspace{10mm} z_{2} = \frac{180 - 175}{10} = 0.5\\
P(Z<0.5) \approx 0.6915, \hspace{10mm} P(Z<-0.5) \approx 0.3085\\
P(-0.5<Z<0.5) = P(Z<0.5) - P(Z<-0.5) = 0.6915 - 0.3085 = 0.3830
\end{gather*}
$$
**Ответ:** Вероятность того, что рост находится между 170 и 180 см, составляет около 38.30%.

4. *Рост, который превышает 90% мужчин.*
Из таблицы стандартного нормального распределения находим, что $P(Z<z) = 0.9$ соответствует $z \approx 1.2816$.
$$
X= \mu + z \cdot \sigma = 175 + 1.2816 \cdot 10 \approx 175 + 12.816 = 187.816
$$
**Ответ:** Рост, который превышает 90% мужчин, составляет около 187.82 см.

<br><br>

### Приближение Пуассона
**Приближение Пуассона** — это полезный метод в теории вероятностей, который позволяет аппроксимировать биномиальное распределение (или другие распределения) с помощью распределения Пуассона. Это особенно удобно, когда число испытаний $n$ велико, а вероятность успеха $p$ мала.
Распределение Пуассона описывает вероятность наступления редких событий за фиксированный интервал времени или пространства. Оно задаётся формулой:
$$
P(Y=k) = \frac{\lambda^{k} e^{- \lambda}}{k!}, \hspace{10mm} \lambda = np
$$
- $\lambda$ — среднее число событий за интервал,
- $k$ — число событий, вероятность которых мы вычисляем,
- $e$ — основание натурального логарифма ($e \approx 2.71828182846$).

Если в биномиальном распределении:
- $n$ велико (обычно $n\geq 20$),
- $p$ мало (обычно $p\leq 0.05$),
- $\lambda = np$ — конечное число (обычно $\lambda \leq 10$),

то биномиальное распределение можно аппроксимировать распределением Пуассона с параметром $\lambda = np$:
$$
P(X=k) \approx \frac{\lambda^{k} e^{- \lambda}}{k!}
$$

#### Пример
Вероятность того, что деталь окажется бракованной, равна $p=0.01$. В партии из $n=100$ деталей найдите вероятность того, что:
1. Ровно 2 детали бракованные.
2. Не более 2 деталей бракованные.
**Точное решение (биномиальное распределение):**
$$
P(X=2) = \binom{100}{2} (0.01)^{2} (0.99)^{98}
$$
Вычисление этого выражения может быть сложным из-за больших чисел.

**Приближение Пуассона:**
$$
\begin{gather*}
\lambda = np = 100 \cdot 0.01 = 1\\
P(X=2) \approx \frac{1^{2} e^{-1}}{2!} = \frac{e^{-1}}{2}

\end{gather*}
$$

**Вероятность не более 2 бракованных деталей:**
- Используем приближение Пуассона для $k = 0, 1, 2$:

$$
\begin{gather*}
P(X\leq2) \approx P(Y=0) + P(Y=1) + P(Y=2),\\
P(Y=0) = \frac{1^{0} e^{-1}}{0!} = e^{-1},\\
P(Y=1) = \frac{1^{1} e^{-1}}{1!} = e^{-1},\\
P(Y=2) = \frac{1^{2} e^{-1}}{2!} = \frac{e^{-1}}{2},\\
\\
P(X\leq 2) \approx 0.9197
\end{gather*}
$$

<br><br>

### Эмпирическое распределение
Эмпирическое распределение — это функция, которая показывает, как часто каждое значение (или диапазон значений) встречается в выборке. Оно строится на основе данных и является непараметрическим, то есть не предполагает, что данные следуют какому-то конкретному теоретическому распределению (например, нормальному или пуассоновскому).

Эмпирическая функция распределения $F_{n}(x)$ определяется как доля наблюдений в выборке, которые меньше или равны $x$:
$$
F_{n}(x) = \frac{\text{число наблюдений}\leq x}{n}
$$
- $n$ — объём выборки,
- $x$ — значение, для которого вычисляется функция.

<br><br>

### Квантили
$$
\begin{gather*}
Q(p) = (1 - \gamma) x_{(j)} + \gamma x_{(j+1)}\\
j= \lfloor p \cdot n \rfloor, \hspace{10mm} \gamma = p \cdot n - j
\end{gather*}
$$
- $Q(p)$ — квантиль уровня $p$,
- $x_{(j)}$​ и $x_{(j+1)}$​ — два соседних значения в упорядоченной выборке,
- $\gamma$ — весовой коэффициент, который определяет, насколько близко $Q(p)$ находится к $x_{(j)}$​ или $x_{(j+1)}$​.

Квантиль — это значение, которое делит данные на две части: одна часть содержит значения меньше квантиля, а другая — больше. Квантили используются для анализа распределения данных и определения их структуры.

Формально, квантиль уровня $p$ (где $0<p<1$) — это значение $x_{p}$​, для которого выполняется:
$$
P(X\leq x_{p}) = p,
$$
где $X$ — случайная величина, а $P(X\leq x_{p})$ — вероятность того, что $X$ примет значение, меньшее или равное $x_{p}$​.

- **Первый квартиль (Q1)** — квантиль уровня 0.25 (25% данных лежат ниже Q1).
- **Второй квартиль (Q2)** — это медиана (50% данных лежат ниже Q2).
- **Третий квартиль (Q3)** — квантиль уровня 0.75 (75% данных лежат ниже Q3).

#### Пример
Дана упорядоченная выборка из 10 значений:
$$
{2,3,5,7,8,10,12,15,18,20}
$$
Найдите квантиль уровня $p=0.75$ (третий квартиль).

1. отсортировать выборку по возрастанию
2. $$
\begin{gather*}
n = 10,\ p = 0.75 \hspace{10mm} j= \lfloor 0.75 \cdot 10 \rfloor = 7\\
x_{(7)} = 12, \hspace{10mm} x_{(8)} = 15, \hspace{10mm} \gamma = 0.72 \cdot 10 - 7 = 0.5\\
Q(0.75) = (1-0.5) \cdot 12 + 0.5 \cdot 15 = 13.5
\end{gather*}
$$ 

<br><br>

### Bessel-коррекции
- В формуле используется $\bar{x}$ (среднее выборки), а не истинное среднее генеральной совокупности $\mu$.
- Поскольку $\bar{x}$ вычисляется на основе тех же данных, что и дисперсия, это приводит к **зависимости** между $\bar{x}$ и $s$. Поэтому при работе с выборкой нужно вводить Bessel-коррекции.
	- **Математическое ожидание** — это теоретическое среднее значение случайной величины, которое не зависит от данных.
	- **Среднее выборки** ($\bar{x}$) — это среднее арифметическое конкретных данных, которое используется для оценки математического ожидания.

##### Скорректированное по Бесселю стандартное отклонение
$$
sd_{s}(\vec{x}) = \sqrt{ \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2}
$$

##### Скорректированная по Бесселю дисперсия
$$
\text{var}_{s}(\vec{x}) = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2
$$

##### Скорректированная по Бесселю ковариация
$$
\text{cov}_{s}(\vec{x}, \vec{y}) = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x}) (y_i - \bar{x})
$$

<br><br>

### Функция правдоподобия
Функция правдоподобия – это фундаментальное понятие в статистике, которое используется для оценки параметров распределения на основе наблюдаемых данных.
Пусть у нас есть случайная выборка $X_{1}, X_{2},\dots, X_{n}$, которая состоит из независимых одинаково распределённых случайных величин с плотностью (или функцией вероятности) $f(x|\theta)$, где $\theta$ – неизвестный параметр.
Тогда **функция правдоподобия** определяется как:
$$
L(\theta) = P(X_{1} = x_{1}, X_{2} = x_{2}, \dots , X_{n} = x_{n} | \theta)
$$
Для непрерывного случая:
$$
L(\theta) = \prod_{i=1}^{n} f(x_{i}|\theta)
$$
Она показывает, насколько вероятно было бы наблюдать данную выборку при определённом значении параметра $\theta$.

Часто удобнее работать с **логарифмом** функции правдоподобия:
$$
\ell(\theta) = \ln L(\theta) = \sum_{i=1}^{n} \ln f(x_{i}|\theta)
$$
Такой переход полезен, так как превращает произведение в сумму и упрощает вычисления.

#### Пример
$$
\begin{gather*}
P(X_{i} = 1) = p, \hspace{10mm} P(X_{i} = 0) = 1-p\\
L(p) = \prod_{i=1}^{n} p^{x_{i}} (1-p)^{1-x_{i}}\\
\ell (p) = \sum_{i=1}^{n} x_{i} \ln p + (1-x_{i}) \ln(1-p)\\
\frac{d \ell}{dp} = \sum_{i=1}^{n} \frac{x_{i}}{p} - \frac{1-x_{i}}{1-p} = 0\\
\text{Решаем уравнение и получаем ответ:} \hspace{10mm}
p=\frac{1}{n} \sum_{i=1}^{n} x_{i}
\end{gather*}
$$

<br><br>

### Основные свойства логарифмов
1. Определение логарифма
$$\log_b a = c \iff b^c = a$$

2. Логарифм произведения:
$$\log_b (xy) = \log_b x + \log_b y$$

1. Логарифм частного:
$$\log_b \left(\frac{x}{y}\right) = \log_b x - \log_b y$$

2. Логарифм степени:
$$\log_b (x^k) = k \cdot \log_b x$$

3. Смена основания логарифма:
$$\log_b a = \frac{\log_k a}{\log_k b}$$

4. Логарифм единицы:
$$\log_b 1 = 0$$

5. Логарифм основания:
$$\log_b b = 1$$

6. Логарифм обратного числа:
$$\log_b \left(\frac{1}{x}\right) = -\log_b x$$

<br><br>

### Статистические доверительные интервалы
#### Доверительный интервал для математического ожидания
Для случая, когда **дисперсия** популяции **известна**, доверительный интервал для среднего значения можно вычислить по формуле:
$$
\bar{x} \pm z \cdot \frac{\sigma}{\sqrt{ n }}
$$
- $\bar{x}$ — выборочное среднее,
- $z$ — значение z-критерия, соответствующее заданному уровню доверия (например, для 95% доверительного интервала $z \approx 1.96$),
- $\sigma$ — известная дисперсия популяции,
- $n$ — размер выборки.

Если **дисперсия** популяции **неизвестна** и выборка мала (обычно $n<30$), используется t-распределение ([[t-распределение.pdf]]):
$$
\bar{x} \pm t \cdot \frac{s}{\sqrt{ n }}
$$
- $s$ — выборочная дисперсия,
- $t$ — значение t-критерия, соответствующее заданному уровню доверия и числу степеней свободы $n-1$.

##### Пример
Допустим, ты провел опрос **100 человек** и узнал, что средний вес — **70 кг**, а стандартное отклонение — **10 кг**.
Мы строим **95%-й** доверительный интервал:  
1. $z$ берем из таблицы нормального распределения $z = 1.96$
2. $$
\begin{gather*}
70 \pm 1.96 \cdot \frac{10}{\sqrt{ 100 }} = 70 \pm 1.96\\
\text{Ответ: } [68.04,71.96]
\end{gather*}
$$

#### Доверительный интервал для пропорции:
$$
\hat{p} \pm z \cdot \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}
$$
- $\hat{p}$ — выборочная пропорция,
- $z$ — значение z-критерия для заданного уровня доверия,
- $n$ — размер выборки.

##### Пример
Допустим, ты проводишь опрос и спрашиваешь **100 человек**, нравится ли им новый фильм. **65 человек ответили "да"**. Значит, пропорция положительных ответов: $\hat{p} = \frac{65}{100} = 0.65$
$$
0.65 \pm 1.96 \sqrt{ \frac{0.65 (1-0.65)}{100} } = 0.65 \pm 0.0935
$$
Мы уверены на 95%, что **реальная доля** людей, которым понравился фильм, находится в интервале $[0.5565,0.7435]$.

<br><br>

### Байесовские статистические модели
**Байесовские модели** основываются на принципах байесовского вывода, который использует теорему Байеса для обновления вероятностей гипотезы на основе новых данных. Этот подход отличается от классической статистики, где мы обычно оцениваем параметры модели без учета предыдущих знаний (например, через метод максимального правдоподобия).
$$
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
$$
- $P(A|B)$ - вероятность гипотезы A при наступлении события B
